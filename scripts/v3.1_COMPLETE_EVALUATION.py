#!/usr/bin/env python3
"""
v3.1 IMPROVED í”„ë¡¬í”„íŠ¸ ì™„ì „ ìì²´ í‰ê°€
ì „ì²´ ìƒ˜í”Œ ëŒ€ìƒ ì˜ˆì¸¡ ì •í™•ë„ ì¸¡ì •
"""

import csv
import math
from typing import List, Dict, Tuple

def classify_with_v31_rules(title: str, content: str, sample_id: str) -> Tuple[int, str]:
    """v3.1 IMPROVED ê·œì¹™ìœ¼ë¡œ ë¶„ë¥˜"""
    text = (title + " " + content).lower()
    
    # v3.1 ì¦‰ì‹œ1 íšŒì‚¬ëª… (í•´ì™¸íšŒì‚¬ ì¶”ê°€)
    auto_companies = [
        "í˜„ëŒ€ì°¨", "í˜„ëŒ€ìë™ì°¨", "ê¸°ì•„", "ì‚¼ì„±sdi", "lgì—ë„ˆì§€ì†”ë£¨ì…˜", 
        "í•œì˜¨", "í¬í‹°íˆ¬", "ì±„ë¹„", "í•œêµ­íƒ€ì´ì–´",
        "ë‹›ì‚°", "í˜¼ë‹¤", "í† ìš”íƒ€", "í…ŒìŠ¬ë¼", "byd", "bmw", "í­ìŠ¤ë°”ê²", "gm", "í¬ë“œ"
    ]
    
    # v3.1 ì¦‰ì‹œ1 ì œí’ˆ/ê¸°ìˆ /ì‹œì¥ í‚¤ì›Œë“œ (í™•ì¥)
    auto_keywords = [
        "ì „ê¸°ì°¨", "ev", "suv", "í•˜ì´ë¸Œë¦¬ë“œ", "ììœ¨ì£¼í–‰", "adas", 
        "ì™„ì„±ì°¨", "oem", "ì¶©ì „ì¸í”„ë¼", "ìë™ì°¨ì‹œì¥", "ì „ê¸°ì°¨ì‹œì¥", 
        "ìë™ì°¨ì‚°ì—…", "ì™„ì„±ì°¨ì—…ê³„", "ìë™ì°¨ì—…ê³„", "ì°¨íŒë§¤", "ìë™ì°¨ì—°êµ¬ì›"
    ]
    
    # ì¦‰ì‹œ0 ì£¼ì œ (ìˆœìˆ˜ ë¹„ìë™ì°¨)
    non_auto_topics = [
        "ì •ì¹˜", "êµ­ë°©", "ìš°ì£¼", "ì˜ë£Œ", "êµìœ¡", "ê²Œì„", "ë¬¸í™”",
        "í†µì‹ ", "í¬í„¸", "ìœ í†µ", "ê±´ì„¤", "ì¡°ì„ ", "í•­ê³µ", "ë¶€ë™ì‚°", "ê¸ˆìœµ"
    ]
    
    # ìœ„í—˜ì¼€ì´ìŠ¤ (ë¬´ì¡°ê±´ 0)
    risk_cases = [
        "uam", "í•­ê³µ", "ì„ ë°•", "ìš°ì£¼", "ê°€ì „ë°°í„°ë¦¬", "essë°°í„°ë¦¬", "ì‚°ì—…ìš©ë°°í„°ë¦¬",
        "ì„œë²„ë°˜ë„ì²´", "ìŠ¤ë§ˆíŠ¸í°ë°˜ë„ì²´", "ê²€ìƒ‰ai", "ì±—ë´‡"
    ]
    
    # ìœ„í—˜ì¼€ì´ìŠ¤ ì²´í¬
    for risk in risk_cases:
        if risk in text:
            return 0, f"ìœ„í—˜ì¼€ì´ìŠ¤: {risk}"
    
    # ì¦‰ì‹œ0 ì£¼ì œ ì²´í¬ (ìë™ì°¨ ì–¸ê¸‰ ì—†ìŒ)
    auto_mentioned = any(k in text for k in ["ìë™ì°¨", "ì „ê¸°ì°¨", "ì™„ì„±ì°¨", "ììœ¨ì£¼í–‰"])
    for topic in non_auto_topics:
        if topic in text and not auto_mentioned:
            return 0, f"ì¦‰ì‹œ0ì£¼ì œ: {topic} (ìë™ì°¨ ì–¸ê¸‰ ì—†ìŒ)"
    
    # ì¦‰ì‹œ1 íšŒì‚¬ëª… ì²´í¬
    has_company = any(company in text for company in auto_companies)
    if has_company:
        return 1, f"ì¦‰ì‹œ1: ìë™ì°¨íšŒì‚¬ëª…"
    
    # ì¦‰ì‹œ1 í‚¤ì›Œë“œ ì²´í¬
    has_keyword = any(keyword in text for keyword in auto_keywords)
    if has_keyword:
        return 1, f"ì¦‰ì‹œ1: ìë™ì°¨í‚¤ì›Œë“œ"
    
    # ì •ë¶€ì •ì±… íŠ¹ë³„ê·œì¹™
    gov_keywords = ["ì •ë¶€", "ì •ì±…", "ì§€ì›", "íˆ¬ì"]
    auto_title_keywords = ["ìë™ì°¨", "ì „ê¸°ì°¨", "ì™„ì„±ì°¨", "ììœ¨ì£¼í–‰"]
    
    has_gov = any(k in text for k in gov_keywords)
    has_auto_in_title = any(k in title.lower() for k in auto_title_keywords)
    
    if has_gov and has_auto_in_title:
        return 1, "ì •ë¶€ì •ì±…: ì œëª©ì— ìë™ì°¨ ëª…ì‹œ"
    elif has_gov:
        return 0, "ì •ë¶€ì •ì±…: ì œëª©ì— ìë™ì°¨ ë¯¸ëª…ì‹œ"
    
    # ë°°í„°ë¦¬/ë°˜ë„ì²´ ê·œì¹™
    if "ë°°í„°ë¦¬" in text:
        if any(x in text for x in ["ì „ê¸°ì°¨", "ì°¨ëŸ‰ìš©", "ev"]) or has_company:
            return 1, "ë°°í„°ë¦¬: ì „ê¸°ì°¨ìš©/ìë™ì°¨íšŒì‚¬"
        return 0, "ë°°í„°ë¦¬: ìš©ë„ë¶ˆëª…í™•"
    
    if "ë°˜ë„ì²´" in text:
        if any(x in text for x in ["ì°¨ëŸ‰ìš©", "ììœ¨ì£¼í–‰"]) or has_company:
            return 1, "ë°˜ë„ì²´: ì°¨ëŸ‰ìš©/ìë™ì°¨íšŒì‚¬"
        return 0, "ë°˜ë„ì²´: ìš©ë„ë¶ˆëª…í™•"
    
    # ê¸°ë³¸ê°’: í™•ì‹ ë¶€ì¡±ì‹œ 0
    return 0, "í™•ì‹ ë¶€ì¡±: ë³´ìˆ˜ì ì ‘ê·¼"

def load_samples_from_csv():
    """CSVì—ì„œ ì „ì²´ ìƒ˜í”Œ ë¡œë“œ"""
    samples = []
    try:
        with open('data/samples.csv', 'r', encoding='utf-8') as f:
            reader = csv.DictReader(f)
            for row in reader:
                samples.append({
                    'id': row.get('id', ''),
                    'title': row.get('title', ''),
                    'content': row.get('content', ''),
                    'label': int(row.get('label', 0))
                })
    except Exception as e:
        print(f"CSV ë¡œë“œ ì˜¤ë¥˜: {e}")
        return []
    
    return samples

def evaluate_v31_complete():
    """v3.1 ì „ì²´ ìƒ˜í”Œ ìì²´ í‰ê°€"""
    print("ğŸ§ª v3.1 IMPROVED ì™„ì „ ìì²´ í‰ê°€")
    print("=" * 60)
    
    samples = load_samples_from_csv()
    if not samples:
        print("âŒ ìƒ˜í”Œ ë°ì´í„°ë¥¼ ë¡œë“œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        return
    
    print(f"ğŸ“Š ì´ {len(samples)}ê°œ ìƒ˜í”Œ í‰ê°€ ì‹œì‘...")
    
    results = []
    correct = 0
    total = 0
    false_positives = []
    false_negatives = []
    
    for sample in samples:
        predicted_label, reasoning = classify_with_v31_rules(
            sample['title'], sample['content'], sample['id']
        )
        
        actual_label = sample['label']
        is_correct = predicted_label == actual_label
        
        if is_correct:
            correct += 1
        else:
            if actual_label == 1 and predicted_label == 0:
                false_negatives.append({
                    'id': sample['id'],
                    'title': sample['title'][:60],
                    'reasoning': reasoning
                })
            else:
                false_positives.append({
                    'id': sample['id'], 
                    'title': sample['title'][:60],
                    'reasoning': reasoning
                })
        
        total += 1
        
        # ì§„í–‰ë¥  í‘œì‹œ
        if total % 50 == 0:
            print(f"ì§„í–‰ë¥ : {total}/{len(samples)} ({total/len(samples)*100:.1f}%)")
    
    accuracy = correct / total
    
    print("\n" + "=" * 60)
    print(f"ğŸ“Š v3.1 ìì²´ í‰ê°€ ìµœì¢… ê²°ê³¼:")
    print(f"ì •í™•ë„: {correct}/{total} = {accuracy*100:.1f}%")
    print(f"ì˜¤ë¶„ë¥˜: {total-correct}ê°œ (FN: {len(false_negatives)}, FP: {len(false_positives)})")
    
    # ì ìˆ˜ ê³„ì‚°
    prompt_length = 1057  # v3.1 ê¸¸ì´
    length_score = math.sqrt(1 - (prompt_length / 3000) ** 2)
    final_score = 0.9 * accuracy + 0.1 * length_score
    
    print(f"\nğŸ“ˆ ì ìˆ˜ ì˜ˆì¸¡:")
    print(f"ê¸¸ì´: {prompt_length}ì")
    print(f"ê¸¸ì´ì ìˆ˜: {length_score:.3f}")
    print(f"ìµœì¢…ì ìˆ˜: {final_score:.3f}")
    print(f"vs ì‹¤ì „(0.854): {final_score-0.854:+.3f}")
    
    # ì˜¤ë¶„ë¥˜ ë¶„ì„
    print(f"\nâŒ False Negative ë¶„ì„ (ì‹¤ì œ1â†’ì˜ˆì¸¡0):")
    for i, fn in enumerate(false_negatives[:10]):  # ìƒìœ„ 10ê°œë§Œ
        print(f"  {i+1}. {fn['id']}: {fn['title']} - {fn['reasoning']}")
    if len(false_negatives) > 10:
        print(f"  ... ì™¸ {len(false_negatives)-10}ê°œ")
    
    print(f"\nâŒ False Positive ë¶„ì„ (ì‹¤ì œ0â†’ì˜ˆì¸¡1):")
    for i, fp in enumerate(false_positives[:10]):  # ìƒìœ„ 10ê°œë§Œ
        print(f"  {i+1}. {fp['id']}: {fp['title']} - {fp['reasoning']}")
    if len(false_positives) > 10:
        print(f"  ... ì™¸ {len(false_positives)-10}ê°œ")
    
    return accuracy, final_score, len(false_negatives), len(false_positives)

if __name__ == "__main__":
    accuracy, final_score, fn_count, fp_count = evaluate_v31_complete()
    
    print(f"\nğŸ¯ v3.1 ì„±ëŠ¥ ì˜ˆì¸¡ ê²°ë¡ :")
    print(f"ì˜ˆìƒ ìµœì¢…ì ìˆ˜: {final_score:.3f}")
    
    if final_score > 0.854:
        improvement = final_score - 0.854
        print(f"âœ… í˜„ì¬(0.854)ë³´ë‹¤ {improvement:.3f}ì  í–¥ìƒ ì˜ˆìƒ")
        rank_estimate = max(1, int(250 * (0.854 / final_score)))
        print(f"ğŸš€ ì˜ˆìƒ ìˆœìœ„: ~{rank_estimate}ë“± (í˜„ì¬ 250ë“± ëŒ€ë¹„)")
    else:
        decline = 0.854 - final_score
        print(f"âŒ í˜„ì¬(0.854)ë³´ë‹¤ {decline:.3f}ì  í•˜ë½ ì˜ˆìƒ")
    
    print(f"\nğŸ“Š ì˜¤ë¶„ë¥˜ íŒ¨í„´:")
    print(f"- False Negative: {fn_count}ê°œ (ë†“ì¹œ ìë™ì°¨ ë‰´ìŠ¤)")
    print(f"- False Positive: {fp_count}ê°œ (ì˜ëª» ë¶„ë¥˜í•œ ë¹„ìë™ì°¨ ë‰´ìŠ¤)")