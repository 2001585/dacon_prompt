import pandas as pd
import json
import re
from typing import Dict, List, Tuple
from datetime import datetime

# í‰ê°€ ê·œì¹™ JSON
EVALUATION_RULES = {
    "automotive_keywords": {
        "strong_positive": {
            "keywords": ["í˜„ëŒ€ì°¨", "ê¸°ì•„", "í…ŒìŠ¬ë¼", "BMW", "ë„ìš”íƒ€", "GM", "í¬ë“œ", "BYD", "ë‹›ì‚°", "í˜¼ë‹¤",
                        "ì „ê¸°ì°¨", "í•˜ì´ë¸Œë¦¬ë“œ", "ììœ¨ì£¼í–‰", "ì°¨ëŸ‰ìš© ë°°í„°ë¦¬", "ì°¨ëŸ‰ìš© ë°˜ë„ì²´",
                        "ì¶©ì „ì†Œ", "íƒ€ì´ì–´", "OEM", "ì‹ ì°¨"],
            "weight": 3,
            "description": "ìë™ì°¨ ì œì¡°ì‚¬ ë° í•µì‹¬ ê¸°ìˆ "
        },
        "moderate_positive": {
            "keywords": ["EV", "HEV", "PHEV", "FCV", "ìë™ì°¨", "ì°¨ëŸ‰", "ì°¨ì¢…", "IVI", "ADAS",
                        "E-GMP", "PPE", "SSP", "NACS", "CCS", "AEC-Q", "ISO26262", "NCAP"],
            "weight": 2,
            "description": "ìë™ì°¨ ê´€ë ¨ ê¸°ìˆ  ìš©ì–´"
        },
        "action_keywords": {
            "keywords": ["ì¶œì‹œ", "ì–‘ì‚°", "ì¦ì„¤", "ìƒì‚°", "íˆ¬ì", "ìˆ˜ì£¼", "ê³µê¸‰ê³„ì•½", "íŒë§¤",
                        "ìˆ˜ì¶œì…", "ì‹¤ì ", "ë¦¬ì½œ", "ì¸ì¦"],
            "weight": 2,
            "description": "ìë™ì°¨ ì‚°ì—… í–‰ìœ„"
        },
        "negative_keywords": {
            "keywords": ["ESS", "íƒœì–‘ê´‘", "ê°€ì •ìš©", "UAM", "í•­ê³µ", "ì¡°ì„ ", "ì •ì±…", "ë¬´ì—­",
                        "ì¼ë°˜ ë°°í„°ë¦¬", "ìŠ¤ë§ˆíŠ¸í°", "ë¡œë´‡"],
            "weight": -3,
            "description": "ë¹„ìë™ì°¨ ì‚°ì—…"
        }
    },
    "critical_patterns": {
        "vehicle_specific": {
            "pattern": r"ì°¨ëŸ‰ìš©|ìë™ì°¨ìš©|ì˜¤í† ëª¨í‹°ë¸Œ|for EV|for vehicle",
            "importance": "HIGH",
            "description": "ì°¨ëŸ‰ ì „ìš© ëª…ì‹œ"
        },
        "manufacturer_action": {
            "pattern": r"(í˜„ëŒ€ì°¨|ê¸°ì•„|í…ŒìŠ¬ë¼|BMW).*(ì¶œì‹œ|ìƒì‚°|íŒë§¤|íˆ¬ì)",
            "importance": "HIGH",
            "description": "ì œì¡°ì‚¬ì™€ í–‰ìœ„ ë™ì‹œ ì¶œí˜„"
        }
    },
    "scoring_threshold": {
        "minimum_score": 3,
        "confidence_levels": {
            "high": 5,
            "medium": 3,
            "low": 1
        }
    }
}

class PromptEvaluator:
    def __init__(self, prompt_text: str, prompt_name: str):
        self.prompt = prompt_text
        self.name = prompt_name
        self.results = {
            "name": prompt_name,
            "length": len(prompt_text),
            "test_results": [],
            "accuracy": 0,
            "final_score": 0,
            "detailed_analysis": {}
        }

    def evaluate_single_case(self, title: str, content: str, actual_label: int) -> Dict:
        """ë‹¨ì¼ í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤ í‰ê°€"""
        text = f"{title} {content}".lower()

        # ì ìˆ˜ ê³„ì‚°
        score = 0
        matched_rules = []

        # í‚¤ì›Œë“œ ê¸°ë°˜ ìŠ¤ì½”ì–´ë§
        for category, rules in EVALUATION_RULES["automotive_keywords"].items():
            for keyword in rules["keywords"]:
                if keyword.lower() in text:
                    score += rules["weight"]
                    matched_rules.append({
                        "category": category,
                        "keyword": keyword,
                        "weight": rules["weight"]
                    })

        # íŒ¨í„´ ë§¤ì¹­
        critical_match = False
        for pattern_name, pattern_info in EVALUATION_RULES["critical_patterns"].items():
            if re.search(pattern_info["pattern"], text, re.IGNORECASE):
                critical_match = True
                matched_rules.append({
                    "pattern": pattern_name,
                    "importance": pattern_info["importance"]
                })

        # ìµœì¢… íŒì •
        if critical_match:
            predicted = 1 if score >= 1 else 0
        else:
            predicted = 1 if score >= EVALUATION_RULES["scoring_threshold"]["minimum_score"] else 0

        # í‰ê°€ ê²°ê³¼
        is_correct = predicted == actual_label

        return {
            "title": title[:50] + "..." if len(title) > 50 else title,
            "actual": actual_label,
            "predicted": predicted,
            "score": score,
            "is_correct": is_correct,
            "matched_rules": matched_rules,
            "confidence": self._get_confidence(score)
        }

    def _get_confidence(self, score: int) -> str:
        """ì‹ ë¢°ë„ ë ˆë²¨ ê²°ì •"""
        levels = EVALUATION_RULES["scoring_threshold"]["confidence_levels"]
        if abs(score) >= levels["high"]:
            return "HIGH"
        elif abs(score) >= levels["medium"]:
            return "MEDIUM"
        else:
            return "LOW"

    def run_evaluation(self, df: pd.DataFrame) -> Dict:
        """ì „ì²´ í‰ê°€ ì‹¤í–‰"""
        print(f"\n{'='*60}")
        print(f"í‰ê°€ ì‹œì‘: {self.name}")
        print(f"í”„ë¡¬í”„íŠ¸ ê¸¸ì´: {self.results['length']}ì")
        print(f"{'='*60}\n")

        correct_count = 0
        wrong_cases = []

        for idx, row in df.iterrows():
            result = self.evaluate_single_case(
                row['Title'],
                row['Content'],
                row['Label']
            )

            self.results["test_results"].append(result)

            # ì½˜ì†” ì¶œë ¥
            status = "âœ… PASS" if result["is_correct"] else "âŒ FAIL"
            confidence = result["confidence"]

            print(f"[{idx:02d}] {status} | ì‹¤ì œ: {result['actual']} | ì˜ˆì¸¡: {result['predicted']} | "
                  f"ì ìˆ˜: {result['score']:+3d} | ì‹ ë¢°ë„: {confidence:6s} | {result['title']}")

            if result["is_correct"]:
                correct_count += 1
            else:
                wrong_cases.append({
                    "index": idx,
                    "title": result["title"],
                    "actual": result["actual"],
                    "predicted": result["predicted"],
                    "score": result["score"]
                })

        # ì •í™•ë„ ê³„ì‚°
        self.results["accuracy"] = correct_count / len(df)

        # ë°ì´ì½˜ ì ìˆ˜ ê³„ì‚°
        length_score = max(0, 1 - (self.results["length"] - 300) / 2700) if self.results["length"] > 300 else 1
        self.results["final_score"] = 0.9 * self.results["accuracy"] + 0.1 * length_score

        # ìƒì„¸ ë¶„ì„
        self.results["detailed_analysis"] = {
            "total_cases": len(df),
            "correct": correct_count,
            "wrong": len(wrong_cases),
            "wrong_cases": wrong_cases,
            "length_score": length_score
        }

        return self.results

    def print_summary(self):
        """í‰ê°€ ìš”ì•½ ì¶œë ¥"""
        print(f"\n{'='*60}")
        print(f"í‰ê°€ ì™„ë£Œ: {self.name}")
        print(f"{'='*60}")
        print(f"ì •í™•ë„: {self.results['accuracy']:.4f} "
              f"({self.results['detailed_analysis']['correct']}/{self.results['detailed_analysis']['total_cases']})")
        print(f"ê¸¸ì´: {self.results['length']}ì")
        print(f"ê¸¸ì´ ì ìˆ˜: {self.results['detailed_analysis']['length_score']:.4f}")
        print(f"ìµœì¢… ì ìˆ˜: {self.results['final_score']:.4f}")

        if self.results['detailed_analysis']['wrong_cases']:
            print(f"\ní‹€ë¦° ì¼€ì´ìŠ¤ ({len(self.results['detailed_analysis']['wrong_cases'])}ê°œ):")
            for case in self.results['detailed_analysis']['wrong_cases'][:5]:  # ì²˜ìŒ 5ê°œë§Œ
                print(f"  - [{case['index']}] {case['title']}")
                print(f"    ì‹¤ì œ: {case['actual']}, ì˜ˆì¸¡: {case['predicted']}, ì ìˆ˜: {case['score']}")

    def save_json_report(self, filename: str):
        """JSON ë¦¬í¬íŠ¸ ì €ì¥"""
        with open(filename, 'w', encoding='utf-8') as f:
            json.dump(self.results, f, ensure_ascii=False, indent=2)
        print(f"\nğŸ“„ JSON ë¦¬í¬íŠ¸ ì €ì¥: {filename}")

def main():
    # í”„ë¡¬í”„íŠ¸ ì •ì˜
    PROMPTS = {
        "Lv1_ë³´ìˆ˜ì _560ì": """[ì—­í• ] ë‰´ìŠ¤í´ë¦¬í•‘ AI: ì…ë ¥ ê¸°ì‚¬ 1ê±´ì´ ìë™ì°¨ì™€ ì§ì ‘ ê´€ë ¨ì¸ì§€ ë¶„ë¥˜.
[ì¶œë ¥] "1" ë˜ëŠ” "0"ë§Œ.
[ì§‘í•©]
A=ì™„ì„±ì°¨Â·OEMÂ·ì „ì¥Â·ë¶€í’ˆÂ·íƒ€ì´ì–´Â·ì¶©ì „Â·ì°¨ëŸ‰ìš©EVë°°í„°ë¦¬
Act=ì¶œì‹œÂ·ì–‘ì‚°Â·ì¦ì„¤Â·ìƒì‚°Â·íˆ¬ìÂ·ìˆ˜ì£¼Â·ê³µê¸‰ê³„ì•½Â·íŒë§¤Â·ìˆ˜ì¶œì…Â·ì‹¤ì Â·ë¦¬ì½œÂ·ì¸ì¦
B=ì •ì±…Â·ë¬´ì—­Â·ê¸ˆìœµÂ·ì™¸êµÂ·ì›ìì¬Â·ì—ë„ˆì§€Â·ESSÂ·ì „ë ¥Â·UAMÂ·í•­ê³µÂ·ì² ë„Â·ì¡°ì„ Â·ë¡œë´‡
[ìŠ¤ì½”ì–´]
+3 ì£¼ì²´ê°€ A
+2 í–‰ìœ„ê°€ Act
+1 ì œëª©: ìë™ì°¨ ì‹ í˜¸(ìë™ì°¨Â·ì°¨ëŸ‰Â·EVÂ·ì°¨ì¢…Â·OEMÂ·IVIÂ·ADAS)
+1 ì°¨ëŸ‰ìš©/ìë™ì°¨ìš©/ì˜¤í† ëª¨í‹°ë¸Œ/AEC-Q/ISO26262/ë¦¬ì½œ/NCAP
+1 Aì™€ Act ë™ì¼ ë¬¸ì¥
+1 EVÂ·HEVÂ·PHEVÂ·FCV/í”Œë«í¼(E-GMPÂ·PPEÂ·SSPÂ·CMF)/ê·œê²©(NACSÂ·CCS)
-3 ì œëª© B ì¤‘ì‹¬(ìë™ì°¨ ì—°ê²° ì—†ìŒ)
-2 ë³¸ë¬¸ B ì¤‘ì‹¬(ì§ì ‘ì„± ë¶ˆëª…)
-2 ë°°í„°ë¦¬Â·ë°˜ë„ì²´Â·ì†Œì¬Â·ì—ë„ˆì§€: 'ì°¨ëŸ‰ìš©' ë¶ˆëª…
-1 ìë™ì°¨ í‚¤ì›Œë“œ ë¶€ì°¨ì 
[íŒì • ê·œì¹™]
total = í•©ê³„ ê²Œì´íŠ¸: totalâ‰¥3 ì´ë©´ì„œ (â‘  OEM/ì°¨ì¢…/ì°¨ëŸ‰ìš©/ê·œì œÂ·ì¸ì¦ ì‹ í˜¸ ì¤‘ í•˜ë‚˜ ëª…ì‹œ ë˜ëŠ” â‘¡ Aì™€ Act ë™ì‹œë¬¸ì¥) ì¼ ë•Œë§Œ 1, ê·¸ ì™¸ 0.""",

        "Lv2_ê· í˜•_540ì": """[ì—­í• ] ë‰´ìŠ¤í´ë¦¬í•‘ AI: ì…ë ¥ ê¸°ì‚¬ 1ê±´ì´ ìë™ì°¨ì™€ ì§ì ‘ ê´€ë ¨ì¸ì§€ ë¶„ë¥˜.
[ì¶œë ¥] "1" ë˜ëŠ” "0"ë§Œ.
[ì§‘í•©]
A=ì™„ì„±ì°¨Â·OEMÂ·ì „ì¥Â·ë¶€í’ˆÂ·íƒ€ì´ì–´Â·ì¶©ì „Â·ì°¨ëŸ‰ìš©EVë°°í„°ë¦¬
Act=ì¶œì‹œÂ·ì–‘ì‚°Â·ì¦ì„¤Â·ìƒì‚°Â·íˆ¬ìÂ·ìˆ˜ì£¼Â·ê³µê¸‰ê³„ì•½Â·íŒë§¤Â·ìˆ˜ì¶œì…Â·ì‹¤ì Â·ë¦¬ì½œÂ·ì¸ì¦
B=ì •ì±…Â·ë¬´ì—­Â·ê¸ˆìœµÂ·ì›ìì¬Â·ì—ë„ˆì§€Â·ESSÂ·ì „ë ¥Â·UAMÂ·í•­ê³µÂ·ì² ë„Â·ì¡°ì„ Â·ë¡œë´‡
[ìŠ¤ì½”ì–´]
+3 ì£¼ì²´ê°€ A
+2 í–‰ìœ„ê°€ Act
+1 ì œëª©: ìë™ì°¨ ì‹ í˜¸(ìë™ì°¨Â·ì°¨ëŸ‰Â·EVÂ·ì°¨ì¢…Â·OEMÂ·IVIÂ·ADAS)
+1 ì°¨ëŸ‰ìš©/ìë™ì°¨ìš©/ì˜¤í† ëª¨í‹°ë¸Œ/AEC-Q/ISO26262/ë¦¬ì½œ/NCAP
+1 Aì™€ Act ë™ì¼ ë¬¸ì¥
+1 EVÂ·HEVÂ·PHEVÂ·FCV/í”Œë«í¼(E-GMPÂ·PPEÂ·SSP)/ê·œê²©(NACSÂ·CCS)
-3 ì œëª© B ì¤‘ì‹¬(ìë™ì°¨ ì—°ê²° ì—†ìŒ)
-2 ë³¸ë¬¸ B ì¤‘ì‹¬(ì§ì ‘ì„± ë¶ˆëª…)
-2 ë°°í„°ë¦¬Â·ë°˜ë„ì²´Â·ì†Œì¬: 'ì°¨ëŸ‰ìš©' ë¶ˆëª…
-1 ìë™ì°¨ í‚¤ì›Œë“œ ë¶€ì°¨ì 
[íŒì •]
totalâ‰¥3 ì´ë©´ì„œ (OEM/ì°¨ì¢…/ì°¨ëŸ‰ìš©/ê·œì œÂ·ì¸ì¦ ì¤‘ í•˜ë‚˜ ë˜ëŠ” Aì™€Act ë™ì‹œ) ì¼ ë•Œë§Œ 1, ê·¸ ì™¸ 0""",

        "Lv3_ì ê·¹_520ì": """[ì—­í• ] ë‰´ìŠ¤í´ë¦¬í•‘ AI: ìë™ì°¨ ì§ì ‘ ê´€ë ¨ ë¶„ë¥˜.
[ì¶œë ¥] "1" ë˜ëŠ” "0"ë§Œ.
[ì§‘í•©]
A=ì™„ì„±ì°¨Â·OEMÂ·ì „ì¥Â·ë¶€í’ˆÂ·íƒ€ì´ì–´Â·ì¶©ì „Â·ì°¨ëŸ‰ìš©ë°°í„°ë¦¬
Act=ì¶œì‹œÂ·ì–‘ì‚°Â·ì¦ì„¤Â·ìƒì‚°Â·íˆ¬ìÂ·ìˆ˜ì£¼Â·ê³µê¸‰ê³„ì•½Â·íŒë§¤Â·ìˆ˜ì¶œì…Â·ì‹¤ì Â·ë¦¬ì½œÂ·ì¸ì¦
B=ì •ì±…Â·ë¬´ì—­Â·ê¸ˆìœµÂ·ì—ë„ˆì§€Â·ESSÂ·UAMÂ·í•­ê³µÂ·ì² ë„Â·ì¡°ì„ Â·ë¡œë´‡
[ìŠ¤ì½”ì–´]
+3 ì£¼ì²´ê°€ A
+2 í–‰ìœ„ê°€ Act
+1 ì œëª©: ì°¨ ì‹ í˜¸(ìë™ì°¨Â·ì°¨ëŸ‰Â·EVÂ·ì°¨ì¢…Â·OEMÂ·IVIÂ·ADAS)
+1 ì°¨ëŸ‰ìš©/ìë™ì°¨ìš©/ì˜¤í† ëª¨í‹°ë¸Œ/AEC-Q/ISO26262/NCAP
+1 Aì™€ Act ë™ì¼ ë¬¸ì¥
+1 EVÂ·HEVÂ·PHEVÂ·FCV/í”Œë«í¼(E-GMPÂ·PPEÂ·SSP)/ì¶©ì „(NACSÂ·CCS)
-3 ì œëª© B ì¤‘ì‹¬
-2 ë³¸ë¬¸ B ì¤‘ì‹¬
-2 ë°°í„°ë¦¬Â·ë°˜ë„ì²´: ì°¨ëŸ‰ìš© ë¶ˆëª…
-1 ì°¨ í‚¤ì›Œë“œ ë¶€ì°¨ì 
[íŒì •]
totalâ‰¥3 ì´ë©´ì„œ (OEM/ì°¨ì¢…/ì°¨ëŸ‰ìš©/ì¸ì¦ ì¤‘ í•˜ë‚˜ ë˜ëŠ” Aì™€Act ë™ì‹œ) ì¼ ë•Œë§Œ 1, ê·¸ ì™¸ 0"""
    }

    # ë°ì´í„° ë¡œë“œ
    print("ğŸ“‚ ë°ì´í„° ë¡œë“œ ì¤‘...")
    df = pd.read_csv('data/samples.csv')
    print(f"âœ… {len(df)}ê°œ ìƒ˜í”Œ ë¡œë“œ ì™„ë£Œ")
    print(f"   - Label 1 (ìë™ì°¨): {sum(df['Label'] == 1)}ê°œ")
    print(f"   - Label 0 (ë¹„ìë™ì°¨): {sum(df['Label'] == 0)}ê°œ")

    # ì „ì²´ ê²°ê³¼ ì €ì¥
    all_results = {
        "evaluation_date": datetime.now().isoformat(),
        "evaluation_rules": EVALUATION_RULES,
        "prompt_results": {}
    }

    # ê° í”„ë¡¬í”„íŠ¸ í‰ê°€
    for prompt_name, prompt_text in PROMPTS.items():
        evaluator = PromptEvaluator(prompt_text, prompt_name)
        results = evaluator.run_evaluation(df)
        evaluator.print_summary()
        all_results["prompt_results"][prompt_name] = results

    # ìµœì¢… ë¹„êµ
    print(f"\n{'='*60}")
    print("ğŸ“Š ìµœì¢… ë¹„êµ")
    print(f"{'='*60}")

    best_prompt = None
    best_score = 0

    for name, results in all_results["prompt_results"].items():
        print(f"{name:20s} | ì ìˆ˜: {results['final_score']:.4f} | "
              f"ì •í™•ë„: {results['accuracy']:.4f} | ê¸¸ì´: {results['length']}ì")

        if results['final_score'] > best_score:
            best_score = results['final_score']
            best_prompt = name

    print(f"\nğŸ† ìµœê³  ì„±ëŠ¥: {best_prompt} (ì ìˆ˜: {best_score:.4f})")

    # ì „ì²´ ê²°ê³¼ JSON ì €ì¥
    with open('results/evaluation_report.json', 'w', encoding='utf-8') as f:
        json.dump(all_results, f, ensure_ascii=False, indent=2)
    print("\nğŸ“„ ì „ì²´ í‰ê°€ ë¦¬í¬íŠ¸ ì €ì¥: results/evaluation_report.json")

    # í‹€ë¦° ì¼€ì´ìŠ¤ ë¶„ì„
    print(f"\n{'='*60}")
    print("ğŸ” ê³µí†µ ì‹¤íŒ¨ íŒ¨í„´ ë¶„ì„")
    print(f"{'='*60}")

    common_failures = {}
    for name, results in all_results["prompt_results"].items():
        for case in results['detailed_analysis']['wrong_cases']:
            idx = case['index']
            if idx not in common_failures:
                common_failures[idx] = {
                    'title': case['title'],
                    'actual': case['actual'],
                    'failed_prompts': []
                }
            common_failures[idx]['failed_prompts'].append(name)

    # ëª¨ë“  í”„ë¡¬í”„íŠ¸ê°€ í‹€ë¦° ì¼€ì´ìŠ¤
    all_failed = [idx for idx, data in common_failures.items()
                  if len(data['failed_prompts']) == len(PROMPTS)]

    if all_failed:
        print(f"ëª¨ë“  í”„ë¡¬í”„íŠ¸ê°€ ì‹¤íŒ¨í•œ ì¼€ì´ìŠ¤ ({len(all_failed)}ê°œ):")
        for idx in all_failed[:3]:  # ì²˜ìŒ 3ê°œë§Œ
            print(f"  - Sample {idx}: {common_failures[idx]['title']}")
            print(f"    ì‹¤ì œ ë¼ë²¨: {common_failures[idx]['actual']}")

if __name__ == "__main__":
    main()
